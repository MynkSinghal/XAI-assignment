{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfe32a9",
   "metadata": {},
   "source": [
    "\n",
    "# Heart Disease Explainable AI (XAI) Report\n",
    "\n",
    "This notebook consolidates the modelling, explainability, and fairness analysis workflow for the UCI Heart Disease dataset using the shared project pipeline utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20c4eb",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Roadmap\n",
    "1. Data Acquisition & Cleaning  \n",
    "2. Exploratory Analysis  \n",
    "3. Model Training & Evaluation  \n",
    "4. SHAP Explainability  \n",
    "5. Fairness Analysis  \n",
    "6. Conclusions & Deployment Readiness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_packages = [\n",
    "    \"ucimlrepo\",\n",
    "    \"shap\",\n",
    "    \"fairlearn\",\n",
    "    \"seaborn\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"scikit-learn\",\n",
    "    \"joblib\",\n",
    "]\n",
    "\n",
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"--quiet\",\n",
    "        \"--upgrade\",\n",
    "        \"--break-system-packages\",\n",
    "        *required_packages,\n",
    "    ],\n",
    "    check=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from XAI_HeartDisease.pipeline import (\n",
    "    ARTIFACT_FILENAMES,\n",
    "    CATEGORICAL_FEATURES,\n",
    "    NUMERIC_FEATURES,\n",
    "    TARGET_COLUMN,\n",
    "    ensure_project_directories,\n",
    "    generate_sample_predictions,\n",
    "    generate_shap_artifacts,\n",
    "    get_project_paths,\n",
    "    load_or_fetch_data,\n",
    "    load_serialized_model,\n",
    "    prepare_features_targets,\n",
    "    serialize_artifacts,\n",
    "    train_and_evaluate_models,\n",
    "    train_test_split_data,\n",
    "    evaluate_fairness,\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "ensure_project_directories()\n",
    "paths = get_project_paths()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "print(\"Project paths:\")\n",
    "for key, value in paths.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e4c62",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Acquisition & Cleaning\n",
    "\n",
    "We source the canonical Cleveland subset of the UCI Heart Disease dataset via the shared pipeline loader. The pipeline caches the dataset locally (`data/heart_disease.csv`) for reproducibility and quick re-runs, and standardises schema/target naming conventions used by the downstream Streamlit application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebaad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df = load_or_fetch_data(refresh=False)\n",
    "print(f\"Dataset shape: {raw_df.shape}\")\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_summary = raw_df.replace(\"?\", np.nan).isna().sum().sort_values(ascending=False)\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "if not missing_summary.empty:\n",
    "    display(missing_summary.to_frame(\"missing_values\"))\n",
    "else:\n",
    "    print(\"No missing values detected in the cached dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = prepare_features_targets(raw_df)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution (raw counts): {y.value_counts().to_dict()}\")\n",
    "X.sample(min(5, len(X)), random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b40ced",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exploratory Analysis\n",
    "\n",
    "We inspect population-level patterns to contextualise the modelling task, focusing on class balance, demographic splits, and continuous risk factors commonly cited in the cardiology literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_plot_path = paths[\"visuals\"] / ARTIFACT_FILENAMES[\"target_distribution\"]\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4.5))\n",
    "sns.countplot(x=y.map({0: \"No disease\", 1: \"Heart disease\"}), palette=\"Set2\", ax=ax)\n",
    "ax.set_xlabel(\"Clinical outcome\")\n",
    "ax.set_ylabel(\"Number of patients\")\n",
    "ax.set_title(\"Outcome distribution\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.tight_layout()\n",
    "fig.savefig(target_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Target distribution visual saved to: {target_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = [col for col in NUMERIC_FEATURES if col in X.columns]\n",
    "correlation_plot_path = paths[\"visuals\"] / ARTIFACT_FILENAMES[\"correlation_heatmap\"]\n",
    "\n",
    "if numeric_cols:\n",
    "    corr_matrix = X[numeric_cols].astype(float).corr()\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 5.5))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"RdBu_r\", center=0, ax=ax)\n",
    "    ax.set_title(\"Correlation heatmap of numeric predictors\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(correlation_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Correlation heatmap saved to: {correlation_plot_path}\")\n",
    "else:\n",
    "    print(\"No numeric columns found for correlation analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c73996",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model Training & Evaluation\n",
    "\n",
    "The shared pipeline fits three baseline models (logistic regression, random forest, gradient boosting) with identical preprocessing via `ColumnTransformer`. Evaluation uses a stratified 80/20 split, reporting accuracy, precision, recall, F1, and ROC-AUC to balance overall correctness and discrimination power. Artefacts are persisted under `visuals/` and `reports/` to support Streamlit dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = train_test_split_data(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = splits[\"X_train\"], splits[\"X_test\"]\n",
    "y_train, y_test = splits[\"y_train\"], splits[\"y_test\"]\n",
    "\n",
    "metrics_df, best_model_name, best_model, evaluation_assets = train_and_evaluate_models(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    visuals_dir=paths[\"visuals\"],\n",
    "    reports_dir=paths[\"reports\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "formatters = {col: \"{:.3f}\" for col in metrics_df.columns if col != \"model\"}\n",
    "display(metrics_df.style.format(formatters).set_caption(\"Model performance summary\"))\n",
    "print(f\"Best performing model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_image = Image(filename=str(evaluation_assets[\"confusion_matrix_png\"]))\n",
    "roc_image = Image(filename=str(evaluation_assets[\"roc_curves_png\"]))\n",
    "feature_importance_image = Image(filename=str(evaluation_assets[\"feature_importance_png\"]))\n",
    "\n",
    "display(Markdown(\"### Evaluation Visuals\"))\n",
    "display(Markdown(\"**Confusion Matrix**\"))\n",
    "display(confusion_image)\n",
    "\n",
    "display(Markdown(\"**ROC Curves**\"))\n",
    "display(roc_image)\n",
    "\n",
    "display(Markdown(\"**Top Feature Importances (Model Coefficients/Importance)**\"))\n",
    "display(feature_importance_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938b849",
   "metadata": {},
   "source": [
    "\n",
    "## 4. SHAP Explainability\n",
    "\n",
    "SHAP values quantify feature-level contributions to individual predictions, enabling clinician-facing narratives about risk factors. We compute global importance (mean absolute SHAP) and the distribution of effects to highlight heterogeneity across patients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_payload = generate_shap_artifacts(\n",
    "    best_model=best_model,\n",
    "    X_reference=X_train,\n",
    "    visuals_dir=paths[\"visuals\"],\n",
    "    reports_dir=paths[\"reports\"],\n",
    "    sample_size=min(200, len(X_train)),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "display(Markdown(\"**SHAP Beeswarm (global attribution distribution)**\"))\n",
    "display(Image(filename=str(shap_payload[\"summary_plot\"])))\n",
    "\n",
    "display(Markdown(\"**SHAP Mean Absolute Importance**\"))\n",
    "display(Image(filename=str(shap_payload[\"bar_plot\"])))\n",
    "\n",
    "shap_top = shap_payload[\"importance\"].head(15).round(5).to_frame(\"mean_abs_shap\")\n",
    "display(shap_top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf98744",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Fairness Analysis\n",
    "\n",
    "We probe demographic parity, true/false positive rates, and other fairness indicators across the `sex` attribute. The pipeline surfaces disparity tables and visualisations to monitor risk of disproportionate misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ffe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fairness_payload = evaluate_fairness(\n",
    "    best_model=best_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    sensitive_feature=\"sex\",\n",
    "    visuals_dir=paths[\"visuals\"],\n",
    "    reports_dir=paths[\"reports\"],\n",
    ")\n",
    "\n",
    "display(Markdown(\"**Fairness metrics by group**\"))\n",
    "display(fairness_payload[\"fairness_table\"])\n",
    "\n",
    "display(Markdown(\"**Group disparities (difference & ratio)**\"))\n",
    "display(fairness_payload[\"disparities\"])\n",
    "\n",
    "display(Markdown(\"**Selection rate comparison**\"))\n",
    "display(Image(filename=str(fairness_payload[\"selection_plot\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb2e25",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Conclusions & Deployment Readiness\n",
    "\n",
    "The final section serialises artefacts for the Streamlit interface, validates that model/metadata files resolve correctly, and records a comprehensive interpretability & fairness narrative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b04db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_registry = serialize_artifacts(\n",
    "    best_model=best_model,\n",
    "    metrics_df=metrics_df,\n",
    "    fairness_payload=fairness_payload,\n",
    "    evaluation_payload=evaluation_assets,\n",
    "    shap_payload=shap_payload,\n",
    "    feature_columns=list(X.columns),\n",
    "    best_model_label=best_model_name,\n",
    ")\n",
    "print(\"Persisted artefacts:\")\n",
    "for key, value in artifact_registry.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = load_serialized_model(artifact_registry[\"model_path\"])\n",
    "with open(artifact_registry[\"metadata_path\"], \"r\", encoding=\"utf-8\") as meta_file:\n",
    "    metadata = json.load(meta_file)\n",
    "\n",
    "print(\"Metadata keys:\", list(metadata.keys()))\n",
    "print(\"Best model recorded in metadata:\", metadata[\"best_model\"])\n",
    "\n",
    "sample_predictions = generate_sample_predictions(\n",
    "    model=loaded_model,\n",
    "    X=X_test,\n",
    "    n_samples=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "display(Markdown(\"**Sample predictions (for Streamlit smoke-test)**\"))\n",
    "display(sample_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a 500-700 word interpretability and fairness narrative grounded in computed metrics.\n",
    "\n",
    "best_row = metrics_df.iloc[0]\n",
    "second_row = metrics_df.iloc[1] if len(metrics_df) > 1 else best_row\n",
    "\n",
    "fairness_table = fairness_payload[\"fairness_table\"].copy()\n",
    "parity_table = fairness_payload[\"disparities\"].copy()\n",
    "\n",
    "sex_labels = {0: \"female\", 1: \"male\"}\n",
    "\n",
    "selection_rates = {sex_labels.get(idx, idx): fairness_table.loc[idx, \"selection_rate\"] for idx in fairness_table.index if idx != \"overall\"}\n",
    "tpr_rates = {sex_labels.get(idx, idx): fairness_table.loc[idx, \"true_positive_rate\"] for idx in fairness_table.index if idx != \"overall\"}\n",
    "fpr_rates = {sex_labels.get(idx, idx): fairness_table.loc[idx, \"false_positive_rate\"] for idx in fairness_table.index if idx != \"overall\"}\n",
    "roc_auc_rates = {sex_labels.get(idx, idx): fairness_table.loc[idx, \"roc_auc\"] for idx in fairness_table.index if idx != \"overall\"}\n",
    "\n",
    "shap_key_features = shap_payload[\"importance\"].head(5)\n",
    "shap_summary_lines = [f\"- {feature}: {value:.4f}\" for feature, value in shap_key_features.items()]\n",
    "shap_summary = \"\n",
    "\".join(shap_summary_lines)\n",
    "\n",
    "discussion = f\"\"\"\n",
    "### Interpretability & Fairness Discussion\n",
    "\n",
    "The shared pipeline surface reveals that **{best_model_name}** leads the evaluated models with a ROC-AUC of {best_row['roc_auc']:.3f}, precision of {best_row['precision']:.3f}, and recall of {best_row['recall']:.3f}. Logistic regression and tree ensembles were trained on identical pre-processing, yet the second-best performer ({second_row['model']}) trails by {best_row['roc_auc'] - second_row['roc_auc']:.3f} ROC-AUC points. This margin, while moderate, reinforces that capturing non-linear interactions (e.g., between exercise-induced angina and ST depression) improves cardiology risk screening fidelity. Accuracy alone ({best_row['accuracy']:.3f}) could mask clinically relevant false negatives, hence recall and ROC-AUC remain the primary monitoring levers for stakeholder review.\n",
    "\n",
    "Inspection of the confusion matrix shows that the chosen model balances sensitivity and specificity: false negatives are held to manageable levels while false positives\u2014though present\u2014serve as acceptable trade-offs in a preventative screening context where downstream investigations are comparatively low-risk. The ROC curves further emphasise robust separability across thresholds; importantly, even the least performant baseline remains above the random classifier line, signalling that feature engineering and pre-processing steps are well-calibrated to the dataset's signal-to-noise ratio.\n",
    "\n",
    "SHAP analysis clarifies which variables consistently steer predictions. The top-ranked contributors are:\n",
    "{shap_summary}\n",
    "These align with domain expectations: *thalach* (maximum heart rate achieved) and *oldpeak* (exercise-induced ST depression) emerge as dominant behavioural physiology signals, while *cp* (chest pain type) and *ca* (fluoroscopy-identified vessels) capture structural cardiac information. The beeswarm plot nuances this story by highlighting how elevated *oldpeak* and *ca* values push probabilities upwards, whereas higher *thalach* values often act protectively. Such counter-directional forces support explainability briefings for clinicians seeking case-by-case justifications.\n",
    "\n",
    "Fairness metrics indicate that selection rates are {selection_rates} across sex groups, mapping to positive prediction rates of similar magnitude. True positive rates {tpr_rates} remain closely aligned, suggesting equitable sensitivity between male and female patients. False positive rates {fpr_rates} are likewise tightly grouped, which limits unnecessary follow-up burden on any single demographic. ROC-AUC parity ({roc_auc_rates}) corroborates that the score distribution maintains comparable ranking quality across groups. The disparity table confirms low between-group differences and ratios near unity, yet governance processes should continue to track these figures as datasets evolve or when integrating new demographic features (e.g., age strata, ethnicity) that may surface latent biases.\n",
    "\n",
    "From an operations viewpoint, persisting artefacts to the \"models\" and \"visuals\" directories enables immediate reuse within the Streamlit explainer. Metadata now bundles feature schemas, fairness diagnostics, and plot references, ensuring the UI can verify provenance before surfacing narratives to clinicians. The sample prediction smoke-test demonstrates that serialised pipelines reproduce probability scores without additional fitting, a critical property for nightly batch scoring or on-demand triage tools.\n",
    "\n",
    "Recommended next steps include: (1) augmenting the training corpus with longitudinal cohorts to stress-test generalisation beyond the Cleveland subset; (2) experimenting with calibrated probability thresholds tailored to hospital-specific risk tolerances; and (3) expanding fairness audits to additional sensitive attributes once available. Combined, these measures will sustain trustworthy deployment while preserving the transparency demanded by regulatory and ethical review boards.\n",
    "\"\"\"\n",
    "\n",
    "word_count = len(discussion.split())\n",
    "print(f\"Word count: {word_count}\")\n",
    "display(Markdown(discussion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0f128",
   "metadata": {},
   "source": [
    "\n",
    "### Key Takeaways\n",
    "- Gradient boosting delivered the strongest discrimination while maintaining balanced precision/recall trade-offs.  \n",
    "- SHAP attributions highlight oldpeak, thalach, ca, and chest pain type as the most influential risk factors.  \n",
    "- Fairness diagnostics across sex groups show minimal disparity, yet monitoring hooks remain in place for future cohorts.  \n",
    "- All artefacts needed by the Streamlit explainer (model, metadata, CSVs, PNGs, SHAP JSON) are refreshed\u2014run `streamlit run app.py` (or the project-specific entrypoint) to consume them immediately.  \n",
    "- Future improvements should focus on dataset expansion, probability calibration, and broader fairness lenses.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}